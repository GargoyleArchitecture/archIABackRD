
from typing import Literal
from langchain_core.messages import AIMessage, SystemMessage, HumanMessage
from langgraph.prebuilt import create_react_agent

from src.graph.state import GraphState
from src.graph.resources import llm, _HAS_VERTEX
from src.graph.consts import prompt_researcher
from src.graph.utils import _push_turn, _last_k_messages, _clip_text
from src.graph.nodes.tools import local_RAG, LLM, LLMWithImages

def researcher_node(state: GraphState) -> GraphState:
    lang = state.get("language", "es")
    intent = state.get("intent", "general")
    force_rag = bool(state.get("force_rag", False))
    doc_only = bool(state.get("doc_only"))
    ctx_doc = (state.get("doc_context") or "").strip()

    # ⛔ GUARD 1: si estamos en turno ASR y NO se forzó RAG, no investigues
    if intent == "asr" and not force_rag:
        note = "(RAG omitido en turno ASR)" if lang == "es" else "(RAG skipped for ASR turn)"
        _push_turn(state, role="assistant", name="researcher", content=note)
        return {
            **state,
            "messages": state["messages"] + [AIMessage(content=note, name="researcher")],
            "hasVisitedInvestigator": True
        }

    # ⛔ GUARD 2: si el intent real es diagrama, este nodo no debe hacer nada
    if intent == "diagram":
        note = "Generando el diagrama con el agente de diagramas…" if lang == "es" else "Diagram will be generated by the diagram agent…"
        _push_turn(state, role="assistant", name="researcher", content=note)
        return {
            **state,
            "messages": state["messages"] + [AIMessage(content=note, name="researcher")],
            "hasVisitedInvestigator": True
        }


    # saludo sin RAG
    if intent in ("greeting", "smalltalk"):
        quick = "Hola, ¿en qué tema de arquitectura te gustaría profundizar?" if lang == "es" \
                else "Hi! How can I help you with software architecture today?"
        _push_turn(state, role="assistant", name="researcher", content=quick)
        return {
            **state,
            "messages": state["messages"] + [AIMessage(content=quick, name="researcher")],
            "hasVisitedInvestigator": True
        }

    # ---- Agente de investigación (con RAG opcional / DOC-ONLY bloquea RAG) ----
    sys = (
        prompt_researcher +
        f"Always reply in {('Spanish' if lang=='es' else 'English')}.\n" +
        ("- If the question is about architecture, you SHOULD call `local_RAG` first to ground your answer, unless force_rag is False."
         if not doc_only else
         "- DOC-ONLY is ON: do NOT call retrieval. Base your answer ONLY on the PROJECT DOCUMENT.\n")
    )

    system_message = SystemMessage(content=sys)
    _push_turn(state, role="system", name="researcher_system", content=sys)

    # Contexto: prioriza doc_context en DOC-ONLY, si no, usa add_context
    ctx_add = (state.get("add_context") or "").strip()
    ctx_for_prompt = ctx_doc if (doc_only and ctx_doc) else ctx_add
    context_message = SystemMessage(
        content=f"PROJECT DOCUMENT (exclusive source):\n{ctx_for_prompt}"
    ) if (doc_only and ctx_for_prompt) else (
        SystemMessage(content=f"PROJECT CONTEXT:\n{ctx_for_prompt}") if ctx_for_prompt else None
    )

    # Herramientas: sin RAG en DOC-ONLY
    tools = ([LLM] + ([LLMWithImages] if _HAS_VERTEX else [])) if doc_only else ([local_RAG, LLM] + ([LLMWithImages] if _HAS_VERTEX else []))
    agent = create_react_agent(llm, tools=tools)

    # HINT corto (solo si no estamos en DOC-ONLY)
    hint_lines = []
    if (force_rag or intent in ("architecture",)) and not doc_only:
        hint_lines.append("Start by calling the tool `local_RAG` with the user's question.")
    if intent == "architecture" and state.get("mermaidCode"):
        hint_lines.append("Also explain the tactics in the provided Mermaid, if any.")
    hint = _clip_text("\n".join(hint_lines).strip(), 100) if hint_lines else ""

    short_history = _last_k_messages(state["messages"], k=6)
    messages_with_system = [system_message] + ([context_message] if context_message else []) + short_history

    payload = {
        "messages": messages_with_system + ([HumanMessage(content=hint)] if hint else []),
        "userQuestion": state["userQuestion"],
        "localQuestion": state["localQuestion"],
        "imagePath1": state["imagePath1"],
        "imagePath2": state["imagePath2"]
    }

    try:
        # Limita la recursión para evitar planeos largos del agente
        result = agent.invoke(payload, config={"recursion_limit": 12})
    except Exception:
        messages_with_system = [system_message] + _last_k_messages(state["messages"], k=3)
        payload["messages"] = messages_with_system
        result = agent.invoke(payload, config={"recursion_limit": 8})

    msgs_out = result.get("messages", [])
    for m in msgs_out:
        _push_turn(state, role="assistant", name="researcher", content=str(getattr(m, "content", m)))
        # NEW: usar la última respuesta del investigador como contexto de negocio/técnico
    if msgs_out:
        last_msg = msgs_out[-1]
        last_text = getattr(last_msg, "content", str(last_msg)) or ""
        # Lo recortamos para no romper el prompt de los siguientes nodos
        state["add_context"] = _clip_text(str(last_text).strip(), 2000)

        return {
        **state,
        "messages": state["messages"] + [
            AIMessage(
                content=str(getattr(m, "content", m)),
                name="researcher"
            ) for m in msgs_out
        ],
        "hasVisitedInvestigator": True
    }
    return state
